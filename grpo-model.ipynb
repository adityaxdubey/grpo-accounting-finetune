{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-04T07:23:02.848887Z",
     "iopub.status.busy": "2025-04-04T07:23:02.848616Z",
     "iopub.status.idle": "2025-04-04T07:27:39.750789Z",
     "shell.execute_reply": "2025-04-04T07:27:39.749998Z",
     "shell.execute_reply.started": "2025-04-04T07:23:02.848864Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.3.19-py3-none-any.whl.metadata (46 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unsloth_zoo>=2025.3.17 (from unsloth)\n",
      "  Downloading unsloth_zoo-2025.3.17-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.1+cu121)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting bitsandbytes (from unsloth)\n",
      "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-0.9.18-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting transformers!=4.47.0,>=4.46.1 (from unsloth)\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.3.1)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.2.1)\n",
      "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n",
      "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.14.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.29.0)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.31.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.20.1+cu121)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.11.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.3.17->unsloth)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.3.17->unsloth) (11.0.0)\n",
      "Collecting torch>=2.4.0 (from unsloth)\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->unsloth) (8.5.0)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from unsloth)\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (4.4.1)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub->unsloth)\n",
      "  Downloading typing_extensions-4.13.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->unsloth) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->unsloth) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n",
      "Downloading unsloth-2025.3.19-py3-none-any.whl (192 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m192.7/192.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.3.17-py3-none-any.whl (127 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl (43.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.18-py3-none-any.whl (123 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading typing_extensions-4.13.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, typing-extensions, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, torch, cut_cross_entropy, transformers, trl, xformers, unsloth_zoo, torchvision, bitsandbytes, unsloth\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
      "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu121\n",
      "    Uninstalling torch-2.5.1+cu121:\n",
      "      Successfully uninstalled torch-2.5.1+cu121\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.0\n",
      "    Uninstalling transformers-4.47.0:\n",
      "      Successfully uninstalled transformers-4.47.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1+cu121\n",
      "    Uninstalling torchvision-0.20.1+cu121:\n",
      "      Successfully uninstalled torchvision-0.20.1+cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\n",
      "langchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\n",
      "pylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\n",
      "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.45.4 cut_cross_entropy-25.1.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 shtab-1.7.1 torch-2.6.0 torchvision-0.21.0 transformers-4.50.3 triton-3.2.0 trl-0.15.2 typing-extensions-4.13.1 tyro-0.9.18 unsloth-2025.3.19 unsloth_zoo-2025.3.17 xformers-0.0.29.post3\n",
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:27:45.477803Z",
     "iopub.status.busy": "2025-04-04T07:27:45.477501Z",
     "iopub.status.idle": "2025-04-04T07:28:13.844661Z",
     "shell.execute_reply": "2025-04-04T07:28:13.842837Z",
     "shell.execute_reply.started": "2025-04-04T07:27:45.477775Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd5aeedf34f44669538e9e7836e054d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f36afb2d57f48779e14069aee0bc28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b288fd1455435895948c583b9e3dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9879e9e749640269344b627c82464eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623bec0806214cdabbc7ccbb634c0c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported  #checking if it works on kaggle t4\n",
    "import torch \n",
    "import re\n",
    "\n",
    "max_seq_length=512  \n",
    "lora_rank=32  \n",
    "\n",
    "#model and tokenizer\n",
    "model, tokenizer=FastLanguageModel.from_pretrained(\n",
    "    model_name=\"meta-llama/meta-Llama-3.1-8B-Instruct\",  \n",
    "    max_seq_length=768, \n",
    "    load_in_4bit=True,  #i used it so as to reduce memory usage\n",
    "    fast_inference=False,  #u may use if you have linux\n",
    "    max_lora_rank=lora_rank,  \n",
    "    gpu_memory_utilization =0.85,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:28:16.996892Z",
     "iopub.status.busy": "2025-04-04T07:28:16.996593Z",
     "iopub.status.idle": "2025-04-04T07:28:24.894151Z",
     "shell.execute_reply": "2025-04-04T07:28:24.893465Z",
     "shell.execute_reply.started": "2025-04-04T07:28:16.996867Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "#peft with lora\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  #this is rank parameter\n",
    "    target_modules=[\n",
    "        \"v_proj\", \"o_proj\",\"q_proj\", \"k_proj\",  #attention\n",
    "        \"gate_proj\", \"down_proj\", \"up_proj\",    # mlp\n",
    "    ],  #layers to be finetuned by lora\n",
    "    lora_alpha = 64,  \n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:28:28.857813Z",
     "iopub.status.busy": "2025-04-04T07:28:28.857453Z",
     "iopub.status.idle": "2025-04-04T07:28:28.904625Z",
     "shell.execute_reply": "2025-04-04T07:28:28.904035Z",
     "shell.execute_reply.started": "2025-04-04T07:28:28.857782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/kaggle/input/dataset/grpo_csv.csv', encoding='ISO-8859-1')\n",
    "\n",
    "df['Question'] = df['Question'].fillna('[EMPTY QUESTION]').astype(str)\n",
    "df['Answer'] = df['Answer'].fillna('<reasoning>[EMPTY ANSWER]</reasoning><answer>[NO ANSWER]</answer>').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:28:32.712804Z",
     "iopub.status.busy": "2025-04-04T07:28:32.712515Z",
     "iopub.status.idle": "2025-04-04T07:28:32.734046Z",
     "shell.execute_reply": "2025-04-04T07:28:32.733097Z",
     "shell.execute_reply.started": "2025-04-04T07:28:32.712783Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Question  \\\n",
      "0  Explain the matching principle and its importa...   \n",
      "1  What is the difference between FIFO and LIFO i...   \n",
      "2  How does the Sarbanes-Oxley Act impact corpora...   \n",
      "3  Explain the concept of present value and its a...   \n",
      "4  What are the key differences between financial...   \n",
      "\n",
      "                                              Answer  \n",
      "0  <reasoning>The matching principle in accountin...  \n",
      "1  <reasoning>FIFO (First-In-First-Out) assumes t...  \n",
      "2  <reasoning>[EMPTY ANSWER]</reasoning><answer>[...  \n",
      "3  <reasoning>Present value is the current worth ...  \n",
      "4  <reasoning>Financial accounting focuses on rep...  \n"
     ]
    }
   ],
   "source": [
    "df.to_csv('/kaggle/working/cleaned_grpo_csv.csv',index=False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:28:34.790458Z",
     "iopub.status.busy": "2025-04-04T07:28:34.790119Z",
     "iopub.status.idle": "2025-04-04T07:28:34.794302Z",
     "shell.execute_reply": "2025-04-04T07:28:34.793459Z",
     "shell.execute_reply.started": "2025-04-04T07:28:34.790428Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT=\"\"\"\n",
    "You MUST format responses EXACTLY like this:\n",
    "\n",
    "<reasoning>\n",
    "[Your detailed analysis here. Minimum 3 sentences]\n",
    "</reasoning>\n",
    "\n",
    "<answer>\n",
    "[Your concise final answer here. Exactly 1 sentence]\n",
    "</answer>\n",
    "\n",
    "FAILURE EXAMPLE (DO NOT DO THIS):\n",
    "The revenue recognition principle states...\n",
    "\n",
    "SUCCESS EXAMPLE:\n",
    "<reasoning>\n",
    "Revenue recognition principles dictate that income should be recorded when...\n",
    "</reasoning>\n",
    "<answer>\n",
    "Revenue is recognized when earned and realizable.</answer>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:28:41.901696Z",
     "iopub.status.busy": "2025-04-04T07:28:41.901405Z",
     "iopub.status.idle": "2025-04-04T07:28:41.907433Z",
     "shell.execute_reply": "2025-04-04T07:28:41.906446Z",
     "shell.execute_reply.started": "2025-04-04T07:28:41.901674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def get_questions_and_answers_dataset(csv_file_path='/kaggle/input/dataset/grpo_csv.csv',max_samples=None)->Dataset:\n",
    "    df = pd.read_csv(csv_file_path,encoding='ISO-8859-1')\n",
    "\n",
    "    df['Question']=df['Question'].fillna('[EMPTY QUESTION]').astype(str)\n",
    "    df['Answer'] = df['Answer'].fillna('<reasoning>[EMPTY ANSWER]</reasoning><answer>[NO ANSWER]</answer>').astype(str)\n",
    "    \n",
    "    #limiting samples\n",
    "    if max_samples is not None and max_samples<len(df):\n",
    "        df=df.head(max_samples)\n",
    "\n",
    "    data_list=[]\n",
    "    for _, row in df.iterrows():\n",
    "        prompt = [\n",
    "            {'role':'system','content':SYSTEM_PROMPT},\n",
    "            {'role': 'user','content':row['Question']}\n",
    "        ]\n",
    "        #reason.  and answer tag format is used for formatted answer variable\n",
    "        formatted_answer= row['Answer']\n",
    "\n",
    "        data_list.append({\n",
    "            'prompt':prompt,\n",
    "            'answer':formatted_answer\n",
    "        })\n",
    "\n",
    "    return Dataset.from_list(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:28:52.996539Z",
     "iopub.status.busy": "2025-04-04T07:28:52.996221Z",
     "iopub.status.idle": "2025-04-04T07:28:53.038895Z",
     "shell.execute_reply": "2025-04-04T07:28:53.038219Z",
     "shell.execute_reply.started": "2025-04-04T07:28:52.996509Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': [{'content': '\\nYou MUST format responses EXACTLY like this:\\n\\n<reasoning>\\n[Your detailed analysis here. Minimum 3 sentences]\\n</reasoning>\\n\\n<answer>\\n[Your concise final answer here. Exactly 1 sentence]\\n</answer>\\n\\nFAILURE EXAMPLE (DO NOT DO THIS):\\nThe revenue recognition principle states...\\n\\nSUCCESS EXAMPLE:\\n<reasoning>\\nRevenue recognition principles dictate that income should be recorded when...\\n</reasoning>\\n<answer>\\nRevenue is recognized when earned and realizable.</answer>\\n', 'role': 'system'}, {'content': 'Explain the matching principle and its importance in financial reporting.', 'role': 'user'}], 'answer': \"<reasoning>The matching principle in accounting requires recording expenses in the same period as the revenues they help generate. For example, depreciation for a machine should be recorded in the accounting period it was used to produce goods. This principle ensures that financial statements accurately reflect the company's performance and prevents misrepresentation of profits or losses.</reasoning><answer>The matching principle ensures accurate financial reporting by aligning expenses with corresponding revenues.</answer>\"}\n"
     ]
    }
   ],
   "source": [
    "dataset = get_questions_and_answers_dataset('/kaggle/input/dataset/grpo_csv.csv', max_samples=100)  \n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:28:54.699331Z",
     "iopub.status.busy": "2025-04-04T07:28:54.699011Z",
     "iopub.status.idle": "2025-04-04T07:28:54.706595Z",
     "shell.execute_reply": "2025-04-04T07:28:54.705684Z",
     "shell.execute_reply.started": "2025-04-04T07:28:54.699302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_xml_answer_from_output(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the content between <answer> tags from the model's output.\n",
    "    \"\"\"\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "def accounting_terminology_reward(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Rewards responses that use appropriate accounting terminology.\"\"\"\n",
    "    key_terms=['liability', 'asset', 'equity', 'revenue', 'expense', 'balance sheet', \n",
    "                 'income statement', 'cash flow', 'gaap', 'ifrs', 'depreciation', \n",
    "                 'amortization', 'accrual', 'audit', 'financial statement', 'inventory']\n",
    "    \n",
    "    rewards=[]\n",
    "    for completion in completions:\n",
    "        content=completion[0]['content'].lower()\n",
    "        reward=sum(term in content for term in key_terms) * 0.1\n",
    "        rewards.append(min(reward, 0.5))\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n",
    "def semantic_correctness_reward(prompts,completions,answer, **kwargs) -> list[float]:\n",
    "    \"\"\"Quick semantic correctness approximation using word overlap.\"\"\"\n",
    "    rewards=[]\n",
    "    for completion,expected_ans in zip(completions,answer):\n",
    "        generated=set(extract_xml_answer_from_output(completion[0]['content']).lower().split())\n",
    "        expected=set(extract_xml_answer_from_output(expected_ans).lower().split())\n",
    "\n",
    "        if not generated or not expected:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        overlap=generated.intersection(expected)\n",
    "        score=len(overlap)/len(expected)\n",
    "        \n",
    "        rewards.append(min(score, 1.0) * 2.0)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# def accounting_principle_reward(prompts, completions, **kwargs) -> list[float]:\n",
    "#     \"\"\"Efficient principle check with simplified logic.\"\"\"\n",
    "#     principles = {\n",
    "#         'matching': ['matching principle', 'expense recognition', 'accrual'],\n",
    "#         'revenue recognition': ['revenue recognition', 'realized', 'earned'],\n",
    "#         'conservatism': ['conservatism', 'prudence', 'lower of cost'],\n",
    "#         'materiality': ['materiality', 'significant', 'threshold'],\n",
    "#         'going concern': ['going concern', 'operational continuity'],\n",
    "#         'consistency': ['consistency', 'comparable', 'uniform application']\n",
    "#     }\n",
    "    \n",
    "#     rewards = []\n",
    "#     for prompt, completion in zip(prompts, completions):\n",
    "#         question = prompt[-1]['content'].lower()\n",
    "#         response = completion[0]['content'].lower()\n",
    "\n",
    "#         #Default reward if no direct match\n",
    "#         reward = 0.3\n",
    "\n",
    "#         for principle, terms in principles.items():\n",
    "#             if any(term in question for term in terms):\n",
    "#                 if any(term in response for term in terms):\n",
    "#                     reward = 0.5  #immediate full reward on matching principles\n",
    "#                     break\n",
    "#                 else: \n",
    "#                     reward = 0.0  #clear negative reward on missing principle\n",
    "#                     break\n",
    "\n",
    "#         rewards.append(reward)\n",
    "\n",
    "#     return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:28:57.348344Z",
     "iopub.status.busy": "2025-04-04T07:28:57.347997Z",
     "iopub.status.idle": "2025-04-04T07:28:57.356875Z",
     "shell.execute_reply": "2025-04-04T07:28:57.356152Z",
     "shell.execute_reply.started": "2025-04-04T07:28:57.348313Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def accounting_principle_reward(prompts, completions, **kwargs) -> list[float]:\n",
    "    principles = {\n",
    "        'matching': ['matching principle', 'expense recognition', 'accrual', 'period costs', 'cost matching'],\n",
    "        'revenue recognition': ['revenue recognition', 'realized', 'earned', 'performance obligation', 'ASC 606', 'IFRS 15'],\n",
    "        'conservatism': ['conservatism', 'prudence', 'lower of cost', 'asset impairment', 'loss contingency'],\n",
    "        'materiality': ['materiality', 'significant', 'threshold', 'omission impact', 'professional judgment'],\n",
    "        'going concern': ['going concern', 'operational continuity', 'liquidity risk', 'bankruptcy risk'],\n",
    "        'consistency': ['consistency', 'comparable', 'uniform application', 'accounting policy'],\n",
    "        'historical cost': ['historical cost', 'original cost', 'acquisition cost', 'historical basis'],\n",
    "        'fair value': ['fair value', 'market value', 'mark-to-market', 'ASC 820', 'IFRS 13'],\n",
    "        'full disclosure': ['full disclosure', 'footnote disclosure', 'transparency', 'contingent liability'],\n",
    "        'economic entity': ['economic entity', 'business entity', 'separate entity', 'consolidation'],\n",
    "        'inventory valuation': ['FIFO', 'LIFO', 'weighted average', 'net realizable value', 'lower cost'],\n",
    "        'lease accounting': ['ASC 842', 'IFRS 16', 'right-of-use', 'lease liability', 'operating lease'],\n",
    "        'impairment': ['impairment loss', 'recoverable amount', 'CGU', 'indefinite-lived assets'],\n",
    "        'foreign currency': ['functional currency', 'translation adjustment', 'spot rate', 'hedging instrument'],\n",
    "        'deferred taxes': ['temporary difference', 'deferred tax asset', 'valuation allowance', 'DTL'],\n",
    "        'segment reporting': ['operating segment', 'ASC 280', 'IFRS 8', 'geographic segment'],\n",
    "        'earnings management': ['earnings per share', 'EBITDA', 'non-GAAP measures', 'pro forma'],\n",
    "        'financial instruments': ['FVTPL', 'FVOCI', 'amortized cost', 'credit risk', 'IFRS 9'],\n",
    "        'business combinations': ['goodwill', 'purchase price allocation', 'contingent consideration', 'IFRS 3'],\n",
    "        'related parties': ['related party transaction', 'arm\\'s length', 'control relationship'],\n",
    "        'subsequent events': ['subsequent events', 'post-balance sheet events', 'adjusting events']\n",
    "    }\n",
    "    \n",
    "    rewards = []\n",
    "    for prompt, completion in zip(prompts, completions):\n",
    "        question = prompt[-1]['content'].lower()\n",
    "        response = completion[0]['content'].lower()\n",
    "        reward = 0.3  #base reward\n",
    "        \n",
    "        #question-principle alignment first\n",
    "        for principle, terms in principles.items():\n",
    "            if any(term in question for term in terms):\n",
    "                # If question mentions principle, require explicit response\n",
    "                if any(term in response for term in terms):\n",
    "                    reward = 0.7  \n",
    "                    break\n",
    "                else:\n",
    "                    reward = 0.1  \n",
    "                    break\n",
    "        else:\n",
    "            #if no specific principle,check implicit coverage\n",
    "            coverage = sum(\n",
    "                0.1 for principle, terms in principles.items()\n",
    "                if any(term in response for term in terms)\n",
    "            )\n",
    "            reward = min(reward + coverage, 0.5)  #0.5max for unsolicited stuff\n",
    "            \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:28:59.247605Z",
     "iopub.status.busy": "2025-04-04T07:28:59.247309Z",
     "iopub.status.idle": "2025-04-04T07:28:59.253710Z",
     "shell.execute_reply": "2025-04-04T07:28:59.253022Z",
     "shell.execute_reply.started": "2025-04-04T07:28:59.247580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    full_pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    partial_patterns = [\n",
    "        (r\"<reasoning>.*?</reasoning>\", 0.5),  #reasoning blockcheck\n",
    "        (r\"<answer>.*?</answer>\", 0.5),#answer block check\n",
    "    ]\n",
    "    \n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        try:\n",
    "            if isinstance(completion, list):\n",
    "                #if list,usefirst element\n",
    "                if len(completion) > 0 and isinstance(completion[0], dict):\n",
    "                    text = completion[0].get(\"content\", \"\")\n",
    "                else:\n",
    "                    text = \"\"\n",
    "            elif isinstance(completion, dict):\n",
    "                #if dict,get content\n",
    "                text = completion.get(\"content\", \"\")\n",
    "            else:\n",
    "                text = \"\"\n",
    "\n",
    "            \n",
    "            #if full xml\n",
    "            if re.search(full_pattern, text, re.DOTALL):\n",
    "                rewards.append(1.0)\n",
    "                continue\n",
    "\n",
    "            partial_reward = 0.0\n",
    "            for pattern, reward_value in partial_patterns:\n",
    "                if re.search(pattern, text, re.DOTALL):\n",
    "                    partial_reward += reward_value\n",
    "\n",
    "            rewards.append(partial_reward)\n",
    "        except (IndexError, KeyError, TypeError) as e:\n",
    "            print(f\"Error processing completion {completion}: {e}\")\n",
    "            rewards.append(0.0)\n",
    "    \n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:29:01.518982Z",
     "iopub.status.busy": "2025-04-04T07:29:01.518643Z",
     "iopub.status.idle": "2025-04-04T07:29:01.553331Z",
     "shell.execute_reply": "2025-04-04T07:29:01.552663Z",
     "shell.execute_reply.started": "2025-04-04T07:29:01.518935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 4 to the `num_generations` of 6\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import pandas as pd\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 2e-4,               \n",
    "    adam_beta1 = 0.9,                   \n",
    "    adam_beta2 = 0.99,               \n",
    "    weight_decay = 0.1,\n",
    "#gradual learning rate warmup over 10% of training\n",
    "    lr_scheduler_type = \"cosine\",      \n",
    "    optim = \"paged_adamw_8bit\",   \n",
    "    logging_steps = 1,             #every step\n",
    "    bf16 = False,      \n",
    "    fp16 = True, \n",
    "    per_device_train_batch_size =4 ,#due to memory constraints\n",
    "    gradient_accumulation_steps = 2,    \n",
    "    num_generations = 6,        \n",
    "    max_prompt_length = 128,     \n",
    "    max_completion_length = 96,\n",
    "\n",
    "    # num_train_epochs = 1,              #commented out in favor of max_steps\n",
    "    max_steps = 100,                 \n",
    "    save_steps = 20,                              \n",
    "    report_to = \"none\",          \n",
    "    output_dir = \"outputs\", \n",
    "    ddp_find_unused_parameters = False,\n",
    "    tf32 = False,  #T4 doesn't support TF32\n",
    "    dataloader_num_workers = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class DebugGRPOTrainer(GRPOTrainer):\n",
    "#     def training_step(self, model, inputs, return_outputs=False):\n",
    "#         # Call the original training step\n",
    "#         loss = super().training_step(model, inputs, return_outputs)\n",
    "\n",
    "#         if self.state.global_step % 5 == 0:  # Every 5 steps, print debug info\n",
    "#             print(f\"\\n--- Training Step {self.state.global_step} ---\")\n",
    "            \n",
    "#             # Check that inputs is a list and attempt to access the completions.\n",
    "#             if isinstance(inputs, list) and len(inputs) > 0 and 'completions' in inputs[0]:\n",
    "#                 completions = inputs[0]['completions']\n",
    "                \n",
    "#                 # Print each completionâ€™s content to examine its structure\n",
    "#                 for i, completion in enumerate(completions[:5]):  # Print up to first 5\n",
    "#                     if isinstance(completion, dict):\n",
    "#                         print(f\"Completion {i}: {repr(completion.get('content', ''))}\")\n",
    "#                     else:\n",
    "#                         print(f\"Completion {i} (unexpected type): {repr(completion)}\")\n",
    "#                     print(\"-\" * 50)\n",
    "                \n",
    "#                 # Now, apply the reward function; it will print each processed text.\n",
    "#                 strict_rewards = strict_format_reward_func(completions)\n",
    "#                 print(f\"Strict Format Rewards: {strict_rewards}\")\n",
    "#             else:\n",
    "#                 print(\"WARNING: 'completions' key not found in inputs or inputs is empty.\")\n",
    "\n",
    "#         return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:29:07.573898Z",
     "iopub.status.busy": "2025-04-04T07:29:07.573595Z",
     "iopub.status.idle": "2025-04-04T07:29:07.578953Z",
     "shell.execute_reply": "2025-04-04T07:29:07.578337Z",
     "shell.execute_reply.started": "2025-04-04T07:29:07.573874Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': [{'content': '\\nYou MUST format responses EXACTLY like this:\\n\\n<reasoning>\\n[Your detailed analysis here. Minimum 3 sentences]\\n</reasoning>\\n\\n<answer>\\n[Your concise final answer here. Exactly 1 sentence]\\n</answer>\\n\\nFAILURE EXAMPLE (DO NOT DO THIS):\\nThe revenue recognition principle states...\\n\\nSUCCESS EXAMPLE:\\n<reasoning>\\nRevenue recognition principles dictate that income should be recorded when...\\n</reasoning>\\n<answer>\\nRevenue is recognized when earned and realizable.</answer>\\n', 'role': 'system'}, {'content': 'Explain the matching principle and its importance in financial reporting.', 'role': 'user'}], 'answer': \"<reasoning>The matching principle in accounting requires recording expenses in the same period as the revenues they help generate. For example, depreciation for a machine should be recorded in the accounting period it was used to produce goods. This principle ensures that financial statements accurately reflect the company's performance and prevents misrepresentation of profits or losses.</reasoning><answer>The matching principle ensures accurate financial reporting by aligning expenses with corresponding revenues.</answer>\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:29:09.400199Z",
     "iopub.status.busy": "2025-04-04T07:29:09.399888Z",
     "iopub.status.idle": "2025-04-04T07:29:09.463642Z",
     "shell.execute_reply": "2025-04-04T07:29:09.462760Z",
     "shell.execute_reply.started": "2025-04-04T07:29:09.400174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from trl import GRPOTrainer\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,          #lora model\n",
    "    processing_class = tokenizer, \n",
    "    reward_funcs = [             \n",
    "        strict_format_reward_func,       #weight: 0.5\n",
    "        #domain-specific rewards\n",
    "        accounting_terminology_reward,   #0.5 \n",
    "        accounting_principle_reward,     \n",
    "        semantic_correctness_reward,  \n",
    "    ],\n",
    "    args = training_args,    #config\n",
    "    train_dataset = dataset,       \n",
    ")\n",
    "trainer.model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:29:11.295472Z",
     "iopub.status.busy": "2025-04-04T07:29:11.295169Z",
     "iopub.status.idle": "2025-04-04T10:18:39.407697Z",
     "shell.execute_reply": "2025-04-04T10:18:39.406808Z",
     "shell.execute_reply.started": "2025-04-04T07:29:11.295447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 90 | Num Epochs = 5 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 12 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (12 x 2 x 1) = 24\n",
      " \"-____-\"     Trainable parameters = 167,772,160/8,000,000,000 (2.10% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 2:47:37, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completion_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / strict_format_reward_func</th>\n",
       "      <th>rewards / accounting_terminology_reward</th>\n",
       "      <th>rewards / accounting_principle_reward</th>\n",
       "      <th>rewards / semantic_correctness_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.594266</td>\n",
       "      <td>0.106997</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.944266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.467130</td>\n",
       "      <td>0.231284</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.879630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.676187</td>\n",
       "      <td>0.118806</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.909521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.497996</td>\n",
       "      <td>0.182587</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245833</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.914663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.430808</td>\n",
       "      <td>0.204487</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.851641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.168616</td>\n",
       "      <td>0.197825</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.023991</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.576950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.955703</td>\n",
       "      <td>0.171064</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.055560</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.179167</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>1.218203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.425316</td>\n",
       "      <td>0.274012</td>\n",
       "      <td>95.125000</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.671149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>2.033202</td>\n",
       "      <td>0.145196</td>\n",
       "      <td>83.708336</td>\n",
       "      <td>0.198155</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.520702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>1.967533</td>\n",
       "      <td>0.224711</td>\n",
       "      <td>61.250000</td>\n",
       "      <td>0.398738</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.592533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>2.080446</td>\n",
       "      <td>0.214465</td>\n",
       "      <td>92.583336</td>\n",
       "      <td>0.355315</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.663779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>1.966468</td>\n",
       "      <td>0.141492</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>0.826265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.329167</td>\n",
       "      <td>0.524802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>2.141915</td>\n",
       "      <td>0.200013</td>\n",
       "      <td>49.208334</td>\n",
       "      <td>0.733462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.650248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>2.301587</td>\n",
       "      <td>0.254239</td>\n",
       "      <td>71.125004</td>\n",
       "      <td>0.400085</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.739087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>2.277050</td>\n",
       "      <td>0.137090</td>\n",
       "      <td>74.041672</td>\n",
       "      <td>0.422449</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.814550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.462651</td>\n",
       "      <td>0.258039</td>\n",
       "      <td>95.375000</td>\n",
       "      <td>0.253594</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.875151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>1.956342</td>\n",
       "      <td>0.285529</td>\n",
       "      <td>88.375000</td>\n",
       "      <td>0.334155</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.793842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>2.007971</td>\n",
       "      <td>0.316708</td>\n",
       "      <td>89.625004</td>\n",
       "      <td>0.307680</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.849638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>2.362681</td>\n",
       "      <td>0.239134</td>\n",
       "      <td>76.375000</td>\n",
       "      <td>0.354291</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.070833</td>\n",
       "      <td>0.445833</td>\n",
       "      <td>0.887681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>2.067285</td>\n",
       "      <td>0.224742</td>\n",
       "      <td>63.458336</td>\n",
       "      <td>0.419714</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.600618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>2.237700</td>\n",
       "      <td>0.186615</td>\n",
       "      <td>55.708334</td>\n",
       "      <td>0.430784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.671033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>2.274377</td>\n",
       "      <td>0.203996</td>\n",
       "      <td>64.458336</td>\n",
       "      <td>0.539767</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.120833</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.766044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>2.202778</td>\n",
       "      <td>0.258825</td>\n",
       "      <td>82.416672</td>\n",
       "      <td>0.486995</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.627778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>2.344203</td>\n",
       "      <td>0.281868</td>\n",
       "      <td>75.750000</td>\n",
       "      <td>0.589119</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.429167</td>\n",
       "      <td>0.798370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>2.334403</td>\n",
       "      <td>0.252052</td>\n",
       "      <td>86.083336</td>\n",
       "      <td>0.505465</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.191667</td>\n",
       "      <td>0.429167</td>\n",
       "      <td>0.984403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>1.754129</td>\n",
       "      <td>0.209792</td>\n",
       "      <td>94.000004</td>\n",
       "      <td>0.522272</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.045833</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.779129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>2.207450</td>\n",
       "      <td>0.259147</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>0.436629</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.890783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>2.218332</td>\n",
       "      <td>0.318180</td>\n",
       "      <td>85.750000</td>\n",
       "      <td>0.471669</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.826665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>2.094544</td>\n",
       "      <td>0.299338</td>\n",
       "      <td>88.541668</td>\n",
       "      <td>0.522972</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.370833</td>\n",
       "      <td>0.811211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>2.204464</td>\n",
       "      <td>0.190394</td>\n",
       "      <td>82.833336</td>\n",
       "      <td>0.455304</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.429167</td>\n",
       "      <td>0.650298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>2.083264</td>\n",
       "      <td>0.177963</td>\n",
       "      <td>83.458336</td>\n",
       "      <td>0.425608</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.816598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>2.656662</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.408414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.504167</td>\n",
       "      <td>0.956662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>2.204040</td>\n",
       "      <td>0.121695</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.154167</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.633206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>2.188258</td>\n",
       "      <td>0.119206</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.346159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>0.750758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>2.178192</td>\n",
       "      <td>0.145963</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.439422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.154167</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.665692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>2.081575</td>\n",
       "      <td>0.193133</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.495922</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>0.673242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>1.969952</td>\n",
       "      <td>0.127550</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.491141</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.424119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>2.235376</td>\n",
       "      <td>0.161335</td>\n",
       "      <td>95.833336</td>\n",
       "      <td>0.464677</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.764542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>2.525212</td>\n",
       "      <td>0.278492</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.483025</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>1.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>2.002712</td>\n",
       "      <td>0.173546</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.400763</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.790212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>2.099698</td>\n",
       "      <td>0.227764</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.427827</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>0.853865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>2.082629</td>\n",
       "      <td>0.307792</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.390116</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.924296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>2.191667</td>\n",
       "      <td>0.213595</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.381095</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.854167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>2.245652</td>\n",
       "      <td>0.162119</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.419708</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.841486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>2.223705</td>\n",
       "      <td>0.268690</td>\n",
       "      <td>95.083336</td>\n",
       "      <td>0.479982</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.154167</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.744538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>2.022331</td>\n",
       "      <td>0.207007</td>\n",
       "      <td>88.250000</td>\n",
       "      <td>0.505552</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.613997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>2.190476</td>\n",
       "      <td>0.233751</td>\n",
       "      <td>93.458336</td>\n",
       "      <td>0.515867</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.782143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>2.266785</td>\n",
       "      <td>0.198805</td>\n",
       "      <td>89.875004</td>\n",
       "      <td>0.525874</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.887618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>2.167105</td>\n",
       "      <td>0.165571</td>\n",
       "      <td>88.833336</td>\n",
       "      <td>0.596746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170833</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.671272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>2.488649</td>\n",
       "      <td>0.225784</td>\n",
       "      <td>89.750004</td>\n",
       "      <td>0.556529</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.813649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>2.242422</td>\n",
       "      <td>0.140743</td>\n",
       "      <td>90.958336</td>\n",
       "      <td>0.508314</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.470833</td>\n",
       "      <td>0.667422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>2.320150</td>\n",
       "      <td>0.203654</td>\n",
       "      <td>92.625004</td>\n",
       "      <td>0.509849</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.882650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>2.406859</td>\n",
       "      <td>0.181927</td>\n",
       "      <td>92.166668</td>\n",
       "      <td>0.529852</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.179167</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.740192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>2.516288</td>\n",
       "      <td>0.134277</td>\n",
       "      <td>85.625004</td>\n",
       "      <td>0.543289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.154167</td>\n",
       "      <td>0.404167</td>\n",
       "      <td>0.957955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>2.387361</td>\n",
       "      <td>0.178492</td>\n",
       "      <td>79.916672</td>\n",
       "      <td>0.570965</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.320833</td>\n",
       "      <td>1.033194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2.210421</td>\n",
       "      <td>0.215606</td>\n",
       "      <td>86.875000</td>\n",
       "      <td>0.501039</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.856254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>2.344658</td>\n",
       "      <td>0.232615</td>\n",
       "      <td>87.291668</td>\n",
       "      <td>0.488798</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.179167</td>\n",
       "      <td>0.320833</td>\n",
       "      <td>0.969658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>2.346230</td>\n",
       "      <td>0.226798</td>\n",
       "      <td>83.791668</td>\n",
       "      <td>0.566087</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.762897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>2.551526</td>\n",
       "      <td>0.214186</td>\n",
       "      <td>83.666672</td>\n",
       "      <td>0.492217</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.955692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>2.434597</td>\n",
       "      <td>0.162327</td>\n",
       "      <td>80.208336</td>\n",
       "      <td>0.578941</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.241667</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.888764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>2.322434</td>\n",
       "      <td>0.265716</td>\n",
       "      <td>83.500004</td>\n",
       "      <td>0.510145</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.834935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>2.363158</td>\n",
       "      <td>0.192653</td>\n",
       "      <td>86.750000</td>\n",
       "      <td>0.572291</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.404167</td>\n",
       "      <td>0.879825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>2.195022</td>\n",
       "      <td>0.211645</td>\n",
       "      <td>79.833336</td>\n",
       "      <td>0.556160</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.728355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>2.117542</td>\n",
       "      <td>0.269779</td>\n",
       "      <td>90.708336</td>\n",
       "      <td>0.597150</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.805042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>2.313439</td>\n",
       "      <td>0.206369</td>\n",
       "      <td>81.958336</td>\n",
       "      <td>0.526111</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.245833</td>\n",
       "      <td>0.445833</td>\n",
       "      <td>0.684272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>2.517915</td>\n",
       "      <td>0.123951</td>\n",
       "      <td>77.708336</td>\n",
       "      <td>0.551288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.888749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>2.523521</td>\n",
       "      <td>0.171245</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.638370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.919354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>2.512368</td>\n",
       "      <td>0.130162</td>\n",
       "      <td>66.458336</td>\n",
       "      <td>0.601744</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1.008201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>2.317157</td>\n",
       "      <td>0.099372</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.606544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.808824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>2.524849</td>\n",
       "      <td>0.126571</td>\n",
       "      <td>62.833334</td>\n",
       "      <td>0.685309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.874849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>2.526191</td>\n",
       "      <td>0.146972</td>\n",
       "      <td>74.208336</td>\n",
       "      <td>0.655985</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.872024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>2.559604</td>\n",
       "      <td>0.193452</td>\n",
       "      <td>61.458336</td>\n",
       "      <td>0.793585</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1.092938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>2.429437</td>\n",
       "      <td>0.122662</td>\n",
       "      <td>71.000004</td>\n",
       "      <td>0.737131</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.345833</td>\n",
       "      <td>0.445833</td>\n",
       "      <td>0.637771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>2.402237</td>\n",
       "      <td>0.143233</td>\n",
       "      <td>72.458336</td>\n",
       "      <td>0.603902</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.814737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>2.382911</td>\n",
       "      <td>0.154076</td>\n",
       "      <td>78.875004</td>\n",
       "      <td>0.743425</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.308333</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.662078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>2.736052</td>\n",
       "      <td>0.161690</td>\n",
       "      <td>71.916668</td>\n",
       "      <td>0.579995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.998552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>2.614226</td>\n",
       "      <td>0.164055</td>\n",
       "      <td>77.916668</td>\n",
       "      <td>0.630515</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.947559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>2.510811</td>\n",
       "      <td>0.168020</td>\n",
       "      <td>79.708336</td>\n",
       "      <td>0.605202</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.308333</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.789977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>2.524851</td>\n",
       "      <td>0.223549</td>\n",
       "      <td>82.166672</td>\n",
       "      <td>0.636137</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.345833</td>\n",
       "      <td>0.858185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>2.378661</td>\n",
       "      <td>0.207375</td>\n",
       "      <td>87.833336</td>\n",
       "      <td>0.650632</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.320833</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.803661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>2.279773</td>\n",
       "      <td>0.170811</td>\n",
       "      <td>91.208336</td>\n",
       "      <td>0.572697</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>1.013107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>2.534718</td>\n",
       "      <td>0.209640</td>\n",
       "      <td>85.791668</td>\n",
       "      <td>0.679185</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.838884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>2.516012</td>\n",
       "      <td>0.221059</td>\n",
       "      <td>88.541668</td>\n",
       "      <td>0.643887</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.874345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>2.241667</td>\n",
       "      <td>0.203563</td>\n",
       "      <td>92.083336</td>\n",
       "      <td>0.572398</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.170833</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>2.535054</td>\n",
       "      <td>0.265808</td>\n",
       "      <td>88.125000</td>\n",
       "      <td>0.675077</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.379167</td>\n",
       "      <td>0.985054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>2.468823</td>\n",
       "      <td>0.253986</td>\n",
       "      <td>90.375004</td>\n",
       "      <td>0.558451</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.129167</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>1.022990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>2.772458</td>\n",
       "      <td>0.257874</td>\n",
       "      <td>85.250000</td>\n",
       "      <td>0.609108</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.934958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>2.354419</td>\n",
       "      <td>0.145962</td>\n",
       "      <td>85.500000</td>\n",
       "      <td>0.611710</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.191667</td>\n",
       "      <td>0.370833</td>\n",
       "      <td>0.854419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>2.485763</td>\n",
       "      <td>0.255592</td>\n",
       "      <td>90.666672</td>\n",
       "      <td>0.576785</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.019096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>2.417088</td>\n",
       "      <td>0.180437</td>\n",
       "      <td>91.583336</td>\n",
       "      <td>0.587782</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.958754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>2.411905</td>\n",
       "      <td>0.213584</td>\n",
       "      <td>86.375004</td>\n",
       "      <td>0.535753</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.429167</td>\n",
       "      <td>0.932738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>2.091890</td>\n",
       "      <td>0.245103</td>\n",
       "      <td>90.416672</td>\n",
       "      <td>0.535842</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.308333</td>\n",
       "      <td>0.875223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>2.373376</td>\n",
       "      <td>0.231502</td>\n",
       "      <td>84.083336</td>\n",
       "      <td>0.577657</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.765043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>2.412558</td>\n",
       "      <td>0.220961</td>\n",
       "      <td>87.500004</td>\n",
       "      <td>0.575454</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.329167</td>\n",
       "      <td>1.041725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>2.554775</td>\n",
       "      <td>0.261256</td>\n",
       "      <td>88.666668</td>\n",
       "      <td>0.622764</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.775609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>2.698260</td>\n",
       "      <td>0.210701</td>\n",
       "      <td>87.041672</td>\n",
       "      <td>0.652622</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.370833</td>\n",
       "      <td>0.504167</td>\n",
       "      <td>0.885760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>2.759570</td>\n",
       "      <td>0.176159</td>\n",
       "      <td>80.083336</td>\n",
       "      <td>0.620853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>1.151236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>2.637401</td>\n",
       "      <td>0.156380</td>\n",
       "      <td>81.666668</td>\n",
       "      <td>0.591523</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.245833</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.924901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>2.499412</td>\n",
       "      <td>0.210091</td>\n",
       "      <td>82.083336</td>\n",
       "      <td>0.641392</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.870245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>2.535805</td>\n",
       "      <td>0.255307</td>\n",
       "      <td>82.416668</td>\n",
       "      <td>0.548453</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.179167</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.939971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.019174884571045256, metrics={'train_runtime': 10166.0067, 'train_samples_per_second': 0.236, 'train_steps_per_second': 0.01, 'total_flos': 0.0, 'train_loss': 0.019174884571045256})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T10:51:40.997773Z",
     "iopub.status.busy": "2025-04-04T10:51:40.997410Z",
     "iopub.status.idle": "2025-04-04T10:51:42.465250Z",
     "shell.execute_reply": "2025-04-04T10:51:42.464313Z",
     "shell.execute_reply.started": "2025-04-04T10:51:40.997741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"grpo_finetuned_lora\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"outputs/fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T11:01:17.543371Z",
     "iopub.status.busy": "2025-04-04T11:01:17.543029Z",
     "iopub.status.idle": "2025-04-04T11:01:53.687216Z",
     "shell.execute_reply": "2025-04-04T11:01:53.686118Z",
     "shell.execute_reply.started": "2025-04-04T11:01:17.543343Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/final_model/ (stored 0%)\n",
      "  adding: kaggle/working/final_model/training_args.json (deflated 63%)\n",
      "  adding: kaggle/working/final_model/README.md (deflated 66%)\n",
      "  adding: kaggle/working/final_model/adapter_config.json (deflated 56%)\n",
      "  adding: kaggle/working/final_model/tokenizer_config.json (deflated 94%)\n",
      "  adding: kaggle/working/final_model/special_tokens_map.json (deflated 71%)\n",
      "  adding: kaggle/working/final_model/adapter_model.safetensors (deflated 8%)\n",
      "  adding: kaggle/working/final_model/tokenizer.json (deflated 85%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /kaggle/working/final_model.zip /kaggle/working/final_model\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6900341,
     "sourceId": 11072693,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
